# CTR (Click-Through Rate) Prediction Model

(A) êµ¬ì²´ì  ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨ + API ì„¤ê³„ (ì„œë¹™ ì—”ë“œí¬ì¸íŠ¸, í”¼ì²˜ ìŠ¤í™, ë ˆì´í„´ì‹œ ì˜ˆì¸¡)
(B) ë©€í‹°íƒœìŠ¤í¬ Transformer ëª¨ë¸ì˜ PyTorch/TF êµ¬í˜„ í…œí”Œë¦¿ (í•™ìŠµ ë£¨í”„, ìº˜ë¦¬ë¸Œë ˆì´ì…˜ í¬í•¨)
(C) ì˜¤í”„ë¼ì¸ OPE + DR/IPS ì˜ˆì œ ì½”ë“œ (ë¡œê·¸ë¥¼ ë„£ìœ¼ë©´ ìƒˆë¡œìš´ ì •ì±… ê°€ì¹˜ ì¶”ì •)
(D) ì»¨í…ìŠ¤íŠ¸ ë°´ë”§(Thompson Sampling) + Lagrangian ì˜ˆì‚° ì œì•½ ì…ì°° íŒŒì´í”„ë¼ì¸ ì½”ë“œ ìƒ˜í”Œ

## Data Preprocessing & Normalization

### ë°ì´í„° ì •ê·œí™”ì˜ ì¤‘ìš”ì„±

ë”¥ëŸ¬ë‹ ëª¨ë¸, íŠ¹íˆ CTR ì˜ˆì¸¡ì—ì„œ ë°ì´í„° ì •ê·œí™”ëŠ” ì„±ëŠ¥ í–¥ìƒì— í•µì‹¬ì ì¸ ì—­í• ì„ í•©ë‹ˆë‹¤.

#### Dense Featuresì— StandardScaler ì ìš©í•˜ëŠ” ì´ìœ :

1. **Gradient ì•ˆì •ì„±**
   - ìŠ¤ì¼€ì¼ì´ ë‹¤ë¥¸ featuresë“¤ì´ gradient ê³„ì‚°ì— ë¶ˆê· í˜•ì„ ë§Œë“¦
   - ì •ê·œí™”í•˜ë©´ ëª¨ë“  featuresê°€ ë™ì¼í•œ ìŠ¤ì¼€ì¼ë¡œ í•™ìŠµì— ê¸°ì—¬

2. **í•™ìŠµ ì†ë„ í–¥ìƒ**
   - ê· ì¼í•œ ìŠ¤ì¼€ì¼ë¡œ optimizerê°€ ë” íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜ë ´
   - Learning rate ì¡°ì •ì´ ë” ì‰¬ì›Œì§

3. **Weight ì´ˆê¸°í™” íš¨ê³¼**
   - Xavier/He ì´ˆê¸°í™”ê°€ ì •ê·œí™”ëœ ì…ë ¥ì—ì„œ ë” ì˜ ì‘ë™
   - í™œì„±í™” í•¨ìˆ˜ì˜ saturation ë°©ì§€

4. **Batch Normalizationê³¼ì˜ ì‹œë„ˆì§€**
   - ì…ë ¥ ì •ê·œí™” + Batch Normìœ¼ë¡œ ë” ì•ˆì •ì ì¸ í•™ìŠµ

#### CTR ëª¨ë¸ì—ì„œ íŠ¹ë³„íˆ ì¤‘ìš”í•œ ì´ìœ :

- **Dense features**: ë³´í†µ ë§¤ìš° ë‹¤ì–‘í•œ ë²”ìœ„ (ì¡°íšŒìˆ˜, í´ë¦­ìˆ˜, ê°€ê²© ë“±)
- **Sparse features**: ì„ë² ë”©ìœ¼ë¡œ ì²˜ë¦¬ë˜ë¯€ë¡œ ë³„ë„ ì •ê·œí™” ë¶ˆí•„ìš”
- **Multi-task learning**: ì„œë¡œ ë‹¤ë¥¸ íƒœìŠ¤í¬ ê°„ ê· í˜• ìœ ì§€

### ì‚¬ìš© ë°©ë²•

```bash
# ë°ì´í„° ì „ì²˜ë¦¬ ë° train/val/test ë¶„ë¦¬
python train.py --prepare-data

# ì¼ë°˜ í›ˆë ¨ (ê¸°ì¡´ ë°ì´í„° ìˆìœ¼ë©´ ë¡œë“œ, ì—†ìœ¼ë©´ ìë™ ìƒì„±)
python train.py
```

ì „ì²˜ë¦¬ëœ ë°ì´í„°ëŠ” `./data/processed/` ë””ë ‰í† ë¦¬ì— ì €ì¥ë©ë‹ˆë‹¤:
- `train_data.parquet`: í›ˆë ¨ ë°ì´í„° (ì •ê·œí™” ì™„ë£Œ)
- `val_data.parquet`: ê²€ì¦ ë°ì´í„° (ì •ê·œí™” ì™„ë£Œ)
- `test_data.parquet`: í…ŒìŠ¤íŠ¸ ë°ì´í„° (ì •ê·œí™” ì™„ë£Œ)
- `preprocessors.pkl`: LabelEncodersì™€ StandardScaler

## TODO List - CTR ëª¨ë¸ ê°œì„  ì‚¬í•­

### ğŸ¯ High Priority

- [ ] **Feature Interaction êµ¬í˜„**
  - [ ] Cross features ì¶”ê°€
  - [ ] Factorization Machines (FM) ë ˆì´ì–´ êµ¬í˜„
  - [ ] DeepFM ë˜ëŠ” xDeepFM ì•„í‚¤í…ì²˜ ì ìš©

- [ ] **Class Imbalance ì²˜ë¦¬**
  - [ ] Focal Loss êµ¬í˜„ (CTR ë°ì´í„°ëŠ” ë³´í†µ í´ë¦­ë¥  1-5%)
  - [ ] Weighted Loss ì¶”ê°€
  - [ ] SMOTE ë˜ëŠ” ë‹¤ë¥¸ ìƒ˜í”Œë§ ê¸°ë²• ê²€í† 

- [ ] **Model Calibration (ë³´ì •)**
  - [ ] Platt Scaling êµ¬í˜„
  - [ ] Isotonic Regression ì¶”ê°€
  - [ ] ì˜ˆì¸¡ í™•ë¥ ê³¼ ì‹¤ì œ í´ë¦­ë¥  ì¼ì¹˜ë„ í‰ê°€

### ğŸ”§ Medium Priority

- [ ] **Training ìµœì í™”**
  - [ ] Early Stopping êµ¬í˜„
  - [ ] Model Checkpointing ì‹œìŠ¤í…œ
  - [ ] Learning Rate Scheduling ê°œì„  (CosineAnnealing, ReduceLROnPlateau)
  - [ ] Gradient Clipping ì¶”ê°€

- [ ] **Regularization ê°•í™”**
  - [ ] L2 Regularization ì¶”ê°€
  - [ ] Dropout rate í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
  - [ ] Batch Normalization ë ˆì´ì–´ ê²€í† 

- [ ] **Embedding ìµœì í™”**
  - [ ] Embedding dimension ìë™ ì¡°ì • (rule-based)
  - [ ] Hash Embedding for high cardinality features
  - [ ] Embedding ì´ˆê¸°í™” ë°©ë²• ìµœì í™”

### ğŸš€ Advanced Features

- [ ] **Negative Sampling**
  - [ ] ëŒ€ê·œëª¨ categorical features íš¨ìœ¨ì  ì²˜ë¦¬
  - [ ] Hierarchical Softmax ê²€í† 

- [ ] **Multi-Task Learning í™•ì¥**
  - [ ] CVR (Conversion Rate) ì˜ˆì¸¡ íƒœìŠ¤í¬ ì¶”ê°€
  - [ ] CTR + CVR joint training (MMOE, PLE ë“±)
  - [ ] Task-specific ê°€ì¤‘ì¹˜ í•™ìŠµ

- [ ] **Advanced Architectures**
  - [ ] Attention ë©”ì»¤ë‹ˆì¦˜ ì¶”ê°€
  - [ ] Feature Selection ìë™í™”
  - [ ] Neural Architecture Search (NAS) ì ìš© ê²€í† 

### ğŸ“Š Monitoring & Evaluation

- [ ] **í‰ê°€ ì§€í‘œ í™•ì¥**
  - [ ] Calibration metrics (Brier Score, Reliability Diagram)
  - [ ] Business metrics (Revenue, ROAS)
  - [ ] A/B Test framework ì¤€ë¹„

- [ ] **ëª¨ë¸ í•´ì„ì„±**
  - [ ] Feature Importance ë¶„ì„
  - [ ] SHAP values ê³„ì‚°
  - [ ] ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”
