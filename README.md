# CTR (Click-Through Rate) Prediction Model

(A) êµ¬ì²´ì  ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨ + API ì„¤ê³„ (ì„œë¹™ ì—”ë“œí¬ì¸íŠ¸, í”¼ì²˜ ìŠ¤í™, ë ˆì´í„´ì‹œ ì˜ˆì¸¡)
(B) ë©€í‹°íƒœìŠ¤í¬ Transformer ëª¨ë¸ì˜ PyTorch/TF êµ¬í˜„ í…œí”Œë¦¿ (í•™ìŠµ ë£¨í”„, ìº˜ë¦¬ë¸Œë ˆì´ì…˜ í¬í•¨)
(C) ì˜¤í”„ë¼ì¸ OPE + DR/IPS ì˜ˆì œ ì½”ë“œ (ë¡œê·¸ë¥¼ ë„£ìœ¼ë©´ ìƒˆë¡œìš´ ì •ì±… ê°€ì¹˜ ì¶”ì •)
(D) ì»¨í…ìŠ¤íŠ¸ ë°´ë”§(Thompson Sampling) + Lagrangian ì˜ˆì‚° ì œì•½ ì…ì°° íŒŒì´í”„ë¼ì¸ ì½”ë“œ ìƒ˜í”Œ

## Data Preprocessing & Normalization

### ë°ì´í„° ì •ê·œí™”ì˜ ì¤‘ìš”ì„±

ë”¥ëŸ¬ë‹ ëª¨ë¸, íŠ¹íˆ CTR ì˜ˆì¸¡ì—ì„œ ë°ì´í„° ì •ê·œí™”ëŠ” ì„±ëŠ¥ í–¥ìƒì— í•µì‹¬ì ì¸ ì—­í• ì„ í•©ë‹ˆë‹¤.

#### Dense Featuresì— StandardScaler ì ìš©í•˜ëŠ” ì´ìœ :

1. **Gradient ì•ˆì •ì„±**
   - ìŠ¤ì¼€ì¼ì´ ë‹¤ë¥¸ featuresë“¤ì´ gradient ê³„ì‚°ì— ë¶ˆê· í˜•ì„ ë§Œë“¦
   - ì •ê·œí™”í•˜ë©´ ëª¨ë“  featuresê°€ ë™ì¼í•œ ìŠ¤ì¼€ì¼ë¡œ í•™ìŠµì— ê¸°ì—¬

2. **í•™ìŠµ ì†ë„ í–¥ìƒ**
   - ê· ì¼í•œ ìŠ¤ì¼€ì¼ë¡œ optimizerê°€ ë” íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜ë ´
   - Learning rate ì¡°ì •ì´ ë” ì‰¬ì›Œì§

3. **Weight ì´ˆê¸°í™” íš¨ê³¼**
   - Xavier/He ì´ˆê¸°í™”ê°€ ì •ê·œí™”ëœ ì…ë ¥ì—ì„œ ë” ì˜ ì‘ë™
   - í™œì„±í™” í•¨ìˆ˜ì˜ saturation ë°©ì§€

4. **Batch Normalizationê³¼ì˜ ì‹œë„ˆì§€**
   - ì…ë ¥ ì •ê·œí™” + Batch Normìœ¼ë¡œ ë” ì•ˆì •ì ì¸ í•™ìŠµ

#### CTR ëª¨ë¸ì—ì„œ íŠ¹ë³„íˆ ì¤‘ìš”í•œ ì´ìœ :

- **Dense features**: ë³´í†µ ë§¤ìš° ë‹¤ì–‘í•œ ë²”ìœ„ (ì¡°íšŒìˆ˜, í´ë¦­ìˆ˜, ê°€ê²© ë“±)
- **Sparse features**: ì„ë² ë”©ìœ¼ë¡œ ì²˜ë¦¬ë˜ë¯€ë¡œ ë³„ë„ ì •ê·œí™” ë¶ˆí•„ìš”
- **Multi-task learning**: ì„œë¡œ ë‹¤ë¥¸ íƒœìŠ¤í¬ ê°„ ê· í˜• ìœ ì§€

### ì‚¬ìš© ë°©ë²•

```bash
# ë°ì´í„° ì „ì²˜ë¦¬ ë° train/val/test ë¶„ë¦¬
python train.py --prepare-data

# ì¼ë°˜ í›ˆë ¨ (ê¸°ì¡´ ë°ì´í„° ìˆìœ¼ë©´ ë¡œë“œ, ì—†ìœ¼ë©´ ìë™ ìƒì„±)
python train.py

# RE-SORT ëª¨ë¸ í›ˆë ¨
python train.py --model resort

# Transformer ëª¨ë¸ í›ˆë ¨ (ê¸°ë³¸ê°’)
python train.py --model transformer
```

ì „ì²˜ë¦¬ëœ ë°ì´í„°ëŠ” `./data/processed/` ë””ë ‰í† ë¦¬ì— ì €ì¥ë©ë‹ˆë‹¤:
- `train_data.parquet`: í›ˆë ¨ ë°ì´í„° (ì •ê·œí™” ì™„ë£Œ)
- `val_data.parquet`: ê²€ì¦ ë°ì´í„° (ì •ê·œí™” ì™„ë£Œ)
- `test_data.parquet`: í…ŒìŠ¤íŠ¸ ë°ì´í„° (ì •ê·œí™” ì™„ë£Œ)
- `preprocessors.pkl`: LabelEncodersì™€ StandardScaler

ì‹¤í—˜ ê²°ê³¼ëŠ” ëª¨ë¸ íƒ€ì…ë³„ë¡œ ì €ì¥ë©ë‹ˆë‹¤:
- `./experiments/transformer_YYYYMMDD_HHMMSS/`: Transformer ëª¨ë¸ ì‹¤í—˜
- `./experiments/resort_YYYYMMDD_HHMMSS/`: RE-SORT ëª¨ë¸ ì‹¤í—˜

## Available Models

### 1. MultiTaskTransformer (ê¸°ë³¸ ëª¨ë¸)
- Transformer ê¸°ë°˜ CTR ì˜ˆì¸¡ ëª¨ë¸
- Adaptive embedding dimensions
- Shared MLP with task-specific heads

### 2. RE-SORT (Removing Spurious Correlation)
**ë…¼ë¬¸**: [RE-SORT: Removing Spurious Correlation in Multilevel Interaction for CTR Prediction](https://arxiv.org/abs/2309.14891)

**ì£¼ìš” íŠ¹ì§•**:
- **Multi-scale Retention (MSR)**: Transformerì˜ self-attentionì„ ê°œì„ í•œ retention mechanismìœ¼ë¡œ ë‹¤ì–‘í•œ ë ˆë²¨ì˜ feature interaction í•™ìŠµ
- **Dual Stream Architecture**: 2ê°œì˜ ë³‘ë ¬ MSR ìŠ¤íŠ¸ë¦¼ (Deep & Shallow)ìœ¼ë¡œ global/local íŒ¨í„´ ë™ì‹œ í¬ì°©
- **Feature Selection Module**: Gating mechanismìœ¼ë¡œ spurious correlation ì œê±°
- **XPOS Positional Encoding**: Enhanced rotary position embedding
- **Interaction Aggregation**: Chunk-based bilinear interactionìœ¼ë¡œ íš¨ìœ¨ì ì¸ feature fusion

**ì•„í‚¤í…ì²˜ êµ¬ì„±**:
```
Input Features (Embeddings)
    â†“
Feature Selection (Gating)
    â†“
MSR Stream 1 (Deep) â† Retention Mechanism (Î³ decay)
MSR Stream 2 (Shallow) â† Retention Mechanism (Î³ decay)
    â†“
Interaction Aggregation (Bilinear)
    â†“
CTR Prediction
```

**í•˜ì´í¼íŒŒë¼ë¯¸í„°**:
- MSR layers: 2
- MSR dimension: 32
- Number of heads: 2
- Embedding dimension: 16
- Dropout: 0.1
- Feature Selection hidden units: [64]

**ì¥ì **:
- Spurious correlation ì œê±°ë¡œ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ
- Multi-scale feature interactionìœ¼ë¡œ ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ
- Retention mechanismìœ¼ë¡œ long-range dependency íš¨ê³¼ì  ì²˜ë¦¬

**íŒŒë¼ë¯¸í„° ìˆ˜**: ~34K (ê²½ëŸ‰ ëª¨ë¸)

## TODO List - CTR ëª¨ë¸ ê°œì„  ì‚¬í•­

### ğŸ¯ High Priority

- [ ] **Feature Interaction êµ¬í˜„**
  - [ ] DeepFM ë˜ëŠ” xDeepFM ì•„í‚¤í…ì²˜ ì ìš© (FM + DNNì˜ end-to-end í•™ìŠµ)
  - [ ] DCN v2 (Deep & Cross Network v2) - ëª…ì‹œì  feature crossing
  - [ ] AutoInt - multi-head self-attention ê¸°ë°˜ feature interaction

- [ ] **Class Imbalance ì²˜ë¦¬**
  - [ ] Focal Loss êµ¬í˜„ (gamma=2 ì¶”ì²œ, CTR í´ë¦­ë¥  1-5%)
  - [ ] Class-balanced Loss (effective number ê¸°ë°˜ ë¦¬ìƒ˜í”Œë§)
  - [ ] Negative sampling with hard negative mining

- [ ] **Model Calibration (ë³´ì •)**
  - [ ] Temperature Scaling (ë‹¨ì¼ íŒŒë¼ë¯¸í„°ë¡œ ë¹ ë¥¸ ë³´ì •)
  - [ ] Platt Scaling (ë¡œì§€ìŠ¤í‹± íšŒê·€ ê¸°ë°˜)
  - [ ] Isotonic Regression (ë¹„ëª¨ìˆ˜ì  ë³´ì •)
  - [ ] Expected Calibration Error (ECE) ë° Reliability Diagramìœ¼ë¡œ í‰ê°€

### ğŸ”§ Medium Priority

- [x] **Training ìµœì í™”** (ì´ë¯¸ êµ¬í˜„ë¨)
  - [x] Model Checkpointing ì‹œìŠ¤í…œ (ì„±ëŠ¥ ê¸°ë°˜ ì €ì¥)
  - [x] Learning Rate Scheduling (CosineAnnealingWarmRestarts ì ìš©)
  - [x] Mixed Precision Training (AMP ì‚¬ìš©)
  - [x] Early Stopping with patience (validation loss ê¸°ë°˜)
  - [x] Gradient Clipping (norm=1.0 ì¶”ì²œ)
  - [x] SWA (Stochastic Weight Averaging) - ë§ˆì§€ë§‰ epochë“¤ í‰ê· 

- [x] **Regularization ê°•í™”**
  - [x] L2 Regularization (weight_decay=1e-5 ì ìš©ë¨)
  - [x] Dropout (0.1 ì ìš©ë¨)
  - [x] Label Smoothing (hard labelì„ soft labelë¡œ)
  - [x] Embedding Dropout (sparse featureìš©)
  - ~~DropConnect (weight dropout)~~ - í˜„ì¬ regularizationìœ¼ë¡œ ì¶©ë¶„, ê³¼ì í•© ì‹¬í•  ì‹œ ì‹¤í—˜ ì˜ˆì •

- [x] **Embedding ìµœì í™”**
  - [x] Adaptive Embedding dimension: ë°ì´í„° ê¸°ë°˜ vocab sizeë³„ ì°¨ì› ì„¤ì •
    - gender (vocab=2): 8ì°¨ì›
    - age_group (vocab=8): 8ì°¨ì›
    - inventory_id (vocab=18): 16ì°¨ì›
    - day_of_week (vocab=7): 8ì°¨ì›
    - hour (vocab=24): 16ì°¨ì›
    - 14.4% íŒŒë¼ë¯¸í„° ê°ì†Œ (944 â†’ 808)
  - [ ] Hash Embedding for high cardinality features (>10K vocab)
  - [ ] Shared Embeddings for related features (user_id, session_id ë“±)
  - [ ] Embedding Regularization (L2 on embeddings)

### ğŸš€ Advanced Features

- [ ] **Context-aware Feature Engineering**
  - [ ] Temporal features (ì‹œê°„ëŒ€ë³„ CTR íŒ¨í„´, recency)
  - [ ] User behavior sequence modeling (LSTM/GRU for click history)
  - [ ] Cross-domain features (user profile + item features interaction)

- [ ] **Multi-Task Learning í™•ì¥**
  - [ ] ESMM (Entire Space Multi-Task Model) - CVR ì˜ˆì¸¡ ì¶”ê°€
  - [ ] MMOE (Multi-gate Mixture-of-Experts) - task-specific experts
  - [ ] PLE (Progressive Layered Extraction) - task separation ê°œì„ 
  - [ ] Uncertainty-based task weighting (homoscedastic uncertainty)

- [x] **Advanced Architectures**
  - [x] RE-SORT (Removing Spurious Correlation) - Multi-scale Retention with Feature Selection
    - Multi-scale Retention (MSR) mechanism
    - Dual stream architecture (Deep & Shallow)
    - XPOS positional encoding
    - Chunk-based interaction aggregation
    - ~34K parameters (ê²½ëŸ‰ ëª¨ë¸)
  - [ ] FiBiNET - Bilinear feature interaction
  - [ ] DLRM (Deep Learning Recommendation Model) - Facebook ì•„í‚¤í…ì²˜
  - [ ] BST (Behavior Sequence Transformer) - Transformer for user sequences
  - [ ] Feature Gating Network (FGN) - ë™ì  feature selection

### ğŸ“Š Monitoring & Evaluation

- [x] **í‰ê°€ ì§€í‘œ í™•ì¥**
  - [x] Calibration metrics (Brier Score, ECE, MCE, Calibration Gap)
  - [x] Calibration Curve (ì˜ˆì¸¡ í™•ë¥  vs ì‹¤ì œ í™•ë¥ , 10 bins)
  - [ ] GAUC (Group AUC) - userë³„ AUC í‰ê· 
  - [ ] NDCG@K - ranking quality
  - [ ] Business metrics (CTR, Revenue, ROAS, eCPM)

- [ ] **ì˜¨ë¼ì¸ í‰ê°€ ì¤€ë¹„**
  - [ ] A/B Test framework (treatment/control split)
  - [ ] Interleaving ì‹¤í—˜ ì„¤ê³„
  - [ ] Online model serving latency ì¸¡ì • (<100ms)

- [ ] **ëª¨ë¸ í•´ì„ì„±**
  - [ ] Integrated Gradients (attribution ê¸°ë°˜ importance)
  - [ ] SHAP values (TreeSHAP ë˜ëŠ” DeepSHAP)
  - [ ] Attention weight visualization
  - [ ] Embedding space visualization (t-SNE, UMAP)

### âš¡ ì„±ëŠ¥ ìµœì í™”

- [x] **ì¶”ë¡  ì†ë„ ê°œì„ ** (ì¼ë¶€ êµ¬í˜„ë¨)
  - [x] torch.compile() ì ìš© (PyTorch 2.0+)
  - [x] Mixed Precision Inference
  - [ ] ONNX ë³€í™˜ ë° ìµœì í™”
  - [ ] TensorRT ë˜ëŠ” OpenVINO ê°€ì†
  - [ ] Embedding ì–‘ìí™” (INT8)
  - [ ] Knowledge Distillation (í° ëª¨ë¸ â†’ ì‘ì€ ëª¨ë¸)

- [ ] **ë¶„ì‚° í•™ìŠµ**
  - [ ] DDP (Distributed Data Parallel)
  - [ ] FSDP (Fully Sharded Data Parallel) for large models
  - [ ] Gradient Accumulation (ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ)

### ğŸ”§ ì—”ì§€ë‹ˆì–´ë§ ê°œì„ 

- [x] **ì‹¤í—˜ ê´€ë¦¬**
  - [x] ëª¨ë¸ë³„ ì‹¤í—˜ ë””ë ‰í† ë¦¬ ìë™ ìƒì„± (transformer_*, resort_*)
  - [x] ì„±ëŠ¥ ê¸°ë°˜ ëª¨ë¸ ì €ì¥ ì‹œìŠ¤í…œ
  - [ ] MLflow/Weights&Biases ì—°ë™

- [ ] **ë°ì´í„° íŒŒì´í”„ë¼ì¸**
  - [ ] Feature store ì—°ë™ (Feast, Tecton)
  - [ ] Online feature serving (Redis, DynamoDB)
  - [ ] Feature versioning ì‹œìŠ¤í…œ

- [ ] **ëª¨ë¸ ì„œë¹™**
  - [ ] FastAPI ë˜ëŠ” TorchServe ê¸°ë°˜ API
  - [ ] Model versioning (A/B test ì§€ì›)
  - [ ] Batch prediction pipeline
  - [ ] Cold start ë¬¸ì œ í•´ê²° (default model)


### ì´ë ‡ê²Œ í–ˆëŠ”ë° lightbgmë³´ë‹¤ ë‚®ìŒ
Tabular ë°ì´í„°ì— ìµœì í™” - CTR ë°ì´í„°ì²˜ëŸ¼ categorical + numerical featureê°€ ì„ì¸ í…Œì´ë¸” ë°ì´í„°ì— ë§¤ìš° ê°•ë ¥í•¨
Feature interaction ìë™ í•™ìŠµ - Tree ê¸°ë°˜ì´ë¼ feature ê°„ ë³µì¡í•œ ìƒí˜¸ì‘ìš©ì„ ìë™ìœ¼ë¡œ ì¡ì•„ëƒ„
ì ì€ ë°ì´í„° ì „ì²˜ë¦¬ - Label encodingë§Œìœ¼ë¡œ ì¶©ë¶„
Overfitting ë°©ì§€ - Built-in regularizationì´ ì˜ ë˜ì–´ìˆìŒ
