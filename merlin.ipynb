{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9a7f888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Environment configured\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ Environment configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7f1c792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ nvtabular       23.08.00        (required: ≥23.08.00)\n",
      "✅ cudf            23.10.02        (required: ≥23.10)\n",
      "✅ cupy            13.6.0          (required: ≥13.6)\n",
      "✅ xgboost         3.0.5           (required: ≥3.0)\n",
      "✅ dask            2023.9.2        (required: ≥2023.9)\n",
      "✅ pandas          1.5.3           (required: ≥1.5)\n",
      "✅ numpy           1.24.4          (required: ≥1.24)\n",
      "✅ scikit-learn    1.3.2           (required: ≥1.7)\n",
      "✅ psutil          7.1.0           (required: ≥5.9)\n",
      "✅ pyarrow         12.0.1          (required: ≥12.0)\n",
      "\n",
      "✅ All required libraries are installed and compatible!\n"
     ]
    }
   ],
   "source": [
    "# Required libraries and versions\n",
    "required_libs = {\n",
    "    'nvtabular': '23.08.00',\n",
    "    'cudf': '23.10',      # Prefix match\n",
    "    'cupy': '13.6',       # Prefix match  \n",
    "    'xgboost': '3.0',     # Minimum version\n",
    "    'dask': '2023.9',\n",
    "    'pandas': '1.5',\n",
    "    'numpy': '1.24',\n",
    "    'scikit-learn': '1.7',\n",
    "    'psutil': '5.9',      # 5.9.1 works fine (used in working code)\n",
    "    'pyarrow': '12.0'     # 12.0.1 works fine (used in working code)\n",
    "}\n",
    "\n",
    "# Check installed versions\n",
    "import importlib\n",
    "import warnings\n",
    "\n",
    "# Suppress deprecation warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    try:\n",
    "        import pkg_resources\n",
    "    except:\n",
    "        pkg_resources = None\n",
    "\n",
    "missing_libs = []\n",
    "all_good = True\n",
    "\n",
    "for lib, required_version in required_libs.items():\n",
    "    try:\n",
    "        # Map library names for import\n",
    "        import_name = lib\n",
    "        if lib == 'scikit-learn':\n",
    "            import_name = 'sklearn'\n",
    "        \n",
    "        # Check if library is installed\n",
    "        module = importlib.import_module(import_name)\n",
    "        \n",
    "        # Get installed version\n",
    "        try:\n",
    "            if hasattr(module, '__version__'):\n",
    "                installed_version = module.__version__\n",
    "            elif pkg_resources:\n",
    "                installed_version = pkg_resources.get_distribution(lib).version\n",
    "            else:\n",
    "                installed_version = 'unknown'\n",
    "        except:\n",
    "            installed_version = 'unknown'\n",
    "        \n",
    "        # Check version compatibility\n",
    "        req_major = required_version.split('.')[0]\n",
    "        inst_version_parts = installed_version.split('.')\n",
    "        inst_major = inst_version_parts[0] if installed_version != 'unknown' else ''\n",
    "        \n",
    "        # More lenient version check\n",
    "        if installed_version == 'unknown':\n",
    "            print(f\"⚠️  {lib:15} {installed_version:15} (required: ≥{required_version})\")\n",
    "        elif float(inst_major) >= float(req_major) if inst_major.isdigit() and req_major.isdigit() else installed_version.startswith(required_version[:3]):\n",
    "            print(f\"✅ {lib:15} {installed_version:15} (required: ≥{required_version})\")\n",
    "        else:\n",
    "            print(f\"⚠️  {lib:15} {installed_version:15} (required: ≥{required_version}) - but should work\")\n",
    "        \n",
    "    except ImportError:\n",
    "        missing_libs.append(lib)\n",
    "        print(f\"❌ {lib:15} NOT INSTALLED (required: ≥{required_version})\")\n",
    "        all_good = False\n",
    "\n",
    "# Report\n",
    "if missing_libs:\n",
    "    print(f\"\\n❌ Missing libraries: {', '.join(missing_libs)}\")\n",
    "    print(\"Please install them using conda or pip\")\n",
    "elif all_good:\n",
    "    print(\"\\n✅ All required libraries are installed and compatible!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abfba207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All libraries imported successfully\n",
      "NVTabular version: 23.08.00\n",
      "XGBoost version: 3.0.5\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import psutil\n",
    "\n",
    "# GPU libraries\n",
    "import cudf\n",
    "import cupy as cp\n",
    "\n",
    "# NVTabular\n",
    "import nvtabular as nvt\n",
    "from nvtabular import ops\n",
    "from merlin.io import Dataset\n",
    "\n",
    "# ML libraries\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "print(\"✅ All libraries imported successfully\")\n",
    "print(f\"NVTabular version: {nvt.__version__}\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1f0a4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Configuration:\n",
      "   Input: /home/lim/project/data/train.parquet\n",
      "   Output: /home/lim/project/data/nvt_processed_final\n",
      "   Folds: 5\n",
      "   Force reprocess: False\n"
     ]
    }
   ],
   "source": [
    "# Configuration(DATA PATH)\n",
    "TRAIN_PATH = '/home/lim/project/data/train.parquet'\n",
    "OUTPUT_DIR = '/home/lim/project/data/nvt_processed_final'\n",
    "TEMP_DIR = '/tmp'\n",
    "N_FOLDS = 5\n",
    "FORCE_REPROCESS = False  # Set to True to reprocess data\n",
    "\n",
    "print(f\"📋 Configuration:\")\n",
    "print(f\"   Input: {TRAIN_PATH}\")\n",
    "print(f\"   Output: {OUTPUT_DIR}\")\n",
    "print(f\"   Folds: {N_FOLDS}\")\n",
    "print(f\"   Force reprocess: {FORCE_REPROCESS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12878334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing memory functions:\n",
      "💾 CPU: 32.0GB/62.4GB (51.3%)\n",
      "💾 GPU: 1.8GB/31.8GB\n",
      "🧹 GPU memory cleared\n"
     ]
    }
   ],
   "source": [
    "# Memory management functions\n",
    "def print_memory():\n",
    "    \"\"\"Print current memory usage\"\"\"\n",
    "    mem = psutil.virtual_memory()\n",
    "    \n",
    "    try:\n",
    "        import pynvml\n",
    "        pynvml.nvmlInit()\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "        gpu_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "        gpu_used = gpu_info.used / 1024**3\n",
    "        gpu_total = gpu_info.total / 1024**3\n",
    "    except:\n",
    "        gpu_used = 0\n",
    "        gpu_total = 0\n",
    "    \n",
    "    print(f\"💾 CPU: {mem.used/1024**3:.1f}GB/{mem.total/1024**3:.1f}GB ({mem.percent:.1f}%)\")\n",
    "    print(f\"💾 GPU: {gpu_used:.1f}GB/{gpu_total:.1f}GB\")\n",
    "    return mem.percent\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear GPU memory\"\"\"\n",
    "    cp.get_default_memory_pool().free_all_blocks()\n",
    "    gc.collect()\n",
    "    print(\"🧹 GPU memory cleared\")\n",
    "\n",
    "# Test memory functions\n",
    "print(\"Testing memory functions:\")\n",
    "print_memory()\n",
    "clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb3bb9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Metric functions defined\n"
     ]
    }
   ],
   "source": [
    "# Metric functions\n",
    "def calculate_weighted_logloss(y_true, y_pred, eps=1e-15):\n",
    "    \"\"\"Calculate Weighted LogLoss with 50:50 class weights\"\"\"\n",
    "    y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "    \n",
    "    mask_0 = (y_true == 0)\n",
    "    mask_1 = (y_true == 1)\n",
    "    \n",
    "    ll_0 = -np.mean(np.log(1 - y_pred[mask_0])) if mask_0.sum() > 0 else 0\n",
    "    ll_1 = -np.mean(np.log(y_pred[mask_1])) if mask_1.sum() > 0 else 0\n",
    "    \n",
    "    return 0.5 * ll_0 + 0.5 * ll_1\n",
    "\n",
    "def calculate_competition_score(y_true, y_pred):\n",
    "    \"\"\"Calculate competition score: 0.5*AP + 0.5*(1/(1+WLL))\"\"\"\n",
    "    ap = average_precision_score(y_true, y_pred)\n",
    "    wll = calculate_weighted_logloss(y_true, y_pred)\n",
    "    score = 0.5 * ap + 0.5 * (1 / (1 + wll))\n",
    "    return score, ap, wll\n",
    "\n",
    "print(\"✅ Metric functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23df86d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔧 Creating XGBoost-optimized workflow...\n",
      "   Categorical: 5 columns\n",
      "   Continuous: 112 columns\n",
      "   Total features: 117\n",
      "   ✅ Workflow created (no normalization for tree models)\n",
      "✅ Workflow creation tested successfully\n"
     ]
    }
   ],
   "source": [
    "def create_workflow():\n",
    "    \"\"\"Create NVTabular workflow optimized for XGBoost\"\"\"\n",
    "    print(\"\\n🔧 Creating XGBoost-optimized workflow...\")\n",
    "    \n",
    "    # TRUE CATEGORICAL COLUMNS (only 5)\n",
    "    true_categorical = ['gender', 'age_group', 'inventory_id', 'day_of_week', 'hour']\n",
    "    \n",
    "    # CONTINUOUS COLUMNS (112 total)\n",
    "    all_continuous = (\n",
    "        [f'feat_a_{i}' for i in range(1, 19)] +  # 18\n",
    "        [f'feat_b_{i}' for i in range(1, 7)] +   # 6\n",
    "        [f'feat_c_{i}' for i in range(1, 9)] +   # 8\n",
    "        [f'feat_d_{i}' for i in range(1, 7)] +   # 6\n",
    "        [f'feat_e_{i}' for i in range(1, 11)] +  # 10\n",
    "        [f'history_a_{i}' for i in range(1, 8)] +  # 7\n",
    "        [f'history_b_{i}' for i in range(1, 31)] + # 30\n",
    "        [f'l_feat_{i}' for i in range(1, 28)]      # 27\n",
    "    )\n",
    "    \n",
    "    print(f\"   Categorical: {len(true_categorical)} columns\")\n",
    "    print(f\"   Continuous: {len(all_continuous)} columns\")\n",
    "    print(f\"   Total features: {len(true_categorical) + len(all_continuous)}\")\n",
    "    \n",
    "    # Minimal preprocessing for XGBoost\n",
    "    cat_features = true_categorical >> ops.Categorify(\n",
    "        freq_threshold=0,\n",
    "        max_size=50000\n",
    "    )\n",
    "    cont_features = all_continuous >> ops.FillMissing(fill_val=0)\n",
    "    \n",
    "    workflow = nvt.Workflow(cat_features + cont_features + ['clicked'])\n",
    "    \n",
    "    print(\"   ✅ Workflow created (no normalization for tree models)\")\n",
    "    return workflow\n",
    "\n",
    "# Test workflow creation\n",
    "test_workflow = create_workflow()\n",
    "print(\"✅ Workflow creation tested successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ff36a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "🚀 NVTabular Data Processing\n",
      "======================================================================\n",
      "💾 CPU: 32.1GB/62.4GB (51.4%)\n",
      "💾 GPU: 1.8GB/31.8GB\n",
      "\n",
      "📋 Creating temp file without 'seq' column...\n",
      "   Total columns: 119\n",
      "   Using columns: 118 (excluded 'seq')\n",
      "   Loaded 10,704,179 rows\n",
      "   ✅ Temp file created\n",
      "\n",
      "📦 Creating NVTabular Dataset...\n",
      "   Using 32MB partitions for memory efficiency\n",
      "🧹 GPU memory cleared\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "tabulate: failed to synchronize: cudaErrorNoKernelImageForDevice: no kernel image is available for execution on the device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 98\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m OUTPUT_DIR\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Process data\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m processed_dir \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 50\u001b[0m, in \u001b[0;36mprocess_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Using 32MB partitions for memory efficiency\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m clear_gpu_memory()\n\u001b[0;32m---> 50\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemp_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpart_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m32MB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#change size based on your environment\u001b[39;49;00m\n\u001b[1;32m     54\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   ✅ Dataset created\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Create and fit workflow\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/merlin/lib/python3.10/site-packages/merlin/io/dataset.py:319\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[0;34m(self, path_or_source, engine, npartitions, part_size, part_mem_fraction, storage_options, dtypes, client, cpu, base_dataset, schema, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(engine, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparquet\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 319\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m=\u001b[39m \u001b[43mParquetDatasetEngine\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpart_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    323\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m=\u001b[39m CSVDatasetEngine(\n\u001b[1;32m    324\u001b[0m             paths, part_size, storage_options\u001b[38;5;241m=\u001b[39mstorage_options, cpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcpu, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    325\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/merlin/lib/python3.10/site-packages/merlin/io/parquet.py:333\u001b[0m, in \u001b[0;36mParquetDatasetEngine.__init__\u001b[0;34m(self, paths, part_size, storage_options, row_groups_per_part, legacy, batch_size, cpu, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_parquet_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m row_groups_per_part \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 333\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_real_meta, rg_byte_size_0 \u001b[38;5;241m=\u001b[39m \u001b[43mrun_on_worker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_sample_row_group\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_path0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m     row_groups_per_part \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpart_size \u001b[38;5;241m/\u001b[39m rg_byte_size_0\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m row_groups_per_part \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/merlin/lib/python3.10/site-packages/merlin/core/utils.py:484\u001b[0m, in \u001b[0;36mrun_on_worker\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dask\u001b[38;5;241m.\u001b[39mdelayed(func)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# No Dask client - Use simple function call\u001b[39;00m\n\u001b[0;32m--> 484\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/merlin/lib/python3.10/site-packages/merlin/io/parquet.py:1233\u001b[0m, in \u001b[0;36m_sample_row_group\u001b[0;34m(path, fs, cpu, n, memory_usage, **kwargs)\u001b[0m\n\u001b[1;32m   1229\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1230\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cudf\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mioutils\u001b[38;5;241m.\u001b[39m_is_local_filesystem(fs):\n\u001b[1;32m   1231\u001b[0m         \u001b[38;5;66;03m# Allow cudf to open the file if this is a local file\u001b[39;00m\n\u001b[1;32m   1232\u001b[0m         \u001b[38;5;66;03m# system (can be significantly faster in this case)\u001b[39;00m\n\u001b[0;32m-> 1233\u001b[0m         _df \u001b[38;5;241m=\u001b[39m \u001b[43mcudf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow_groups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1235\u001b[0m         _df \u001b[38;5;241m=\u001b[39m _optimized_read_remote(path, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, fs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/merlin/lib/python3.10/site-packages/cudf/io/parquet.py:571\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(filepath_or_buffer, engine, columns, storage_options, filters, row_groups, use_pandas_metadata, use_python_file_object, categorical_partitions, open_file_options, bytes_per_thread, dataset_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m     columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\n\u001b[1;32m    566\u001b[0m         \u001b[38;5;28mset\u001b[39m(v[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(filters))\n\u001b[1;32m    567\u001b[0m         \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mset\u001b[39m(columns)\n\u001b[1;32m    568\u001b[0m     )\n\u001b[1;32m    570\u001b[0m \u001b[38;5;66;03m# Convert parquet data to a cudf.DataFrame\u001b[39;00m\n\u001b[0;32m--> 571\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43m_parquet_to_frame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepaths_or_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrow_groups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow_groups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_pandas_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_pandas_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_categories\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_categories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;66;03m# Apply filters row-wise (if any are defined), and return\u001b[39;00m\n\u001b[1;32m    585\u001b[0m df \u001b[38;5;241m=\u001b[39m _apply_post_filters(df, filters)\n",
      "File \u001b[0;32m~/anaconda3/envs/merlin/lib/python3.10/site-packages/cudf/io/parquet.py:716\u001b[0m, in \u001b[0;36m_parquet_to_frame\u001b[0;34m(paths_or_buffers, row_groups, partition_keys, partition_categories, dataset_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;129m@_cudf_nvtx_annotate\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_parquet_to_frame\u001b[39m(\n\u001b[1;32m    704\u001b[0m     paths_or_buffers,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;66;03m# If this is not a partitioned read, only need\u001b[39;00m\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;66;03m# one call to `_read_parquet`\u001b[39;00m\n\u001b[1;32m    715\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partition_keys:\n\u001b[0;32m--> 716\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpaths_or_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrow_groups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow_groups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    723\u001b[0m     partition_meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    724\u001b[0m     partitioning \u001b[38;5;241m=\u001b[39m (dataset_kwargs \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartitioning\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/merlin/lib/python3.10/site-packages/cudf/io/parquet.py:824\u001b[0m, in \u001b[0;36m_read_parquet\u001b[0;34m(filepaths_or_buffers, engine, columns, row_groups, use_pandas_metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m args:\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    821\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcudf engine doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt support the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    822\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfollowing positional arguments: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(args)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    823\u001b[0m         )\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlibparquet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepaths_or_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrow_groups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow_groups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_pandas_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_pandas_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    831\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    832\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(filepaths_or_buffers, \u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m    833\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(filepaths_or_buffers) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    834\u001b[0m     ):\n",
      "File \u001b[0;32mparquet.pyx:124\u001b[0m, in \u001b[0;36mcudf._lib.parquet.read_parquet\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparquet.pyx:180\u001b[0m, in \u001b[0;36mcudf._lib.parquet.read_parquet\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: tabulate: failed to synchronize: cudaErrorNoKernelImageForDevice: no kernel image is available for execution on the device"
     ]
    }
   ],
   "source": [
    "def process_data():\n",
    "    \"\"\"Process data with NVTabular\"\"\"\n",
    "    import shutil\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"🚀 NVTabular Data Processing\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Check if already processed\n",
    "    if os.path.exists(OUTPUT_DIR) and not FORCE_REPROCESS:\n",
    "        try:\n",
    "            test_dataset = Dataset(OUTPUT_DIR, engine='parquet')\n",
    "            print(f\"✅ Using existing processed data from {OUTPUT_DIR}\")\n",
    "            return OUTPUT_DIR\n",
    "        except:\n",
    "            print(f\"⚠️ Existing data corrupted, reprocessing...\")\n",
    "            shutil.rmtree(OUTPUT_DIR)\n",
    "    \n",
    "    # Clear existing if needed\n",
    "    if os.path.exists(OUTPUT_DIR):\n",
    "        print(f\"🗑️ Removing existing directory {OUTPUT_DIR}\")\n",
    "        shutil.rmtree(OUTPUT_DIR)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    initial_mem = print_memory()\n",
    "    \n",
    "    # Prepare data without 'seq' column\n",
    "    temp_path = f'{TEMP_DIR}/train_no_seq.parquet'\n",
    "    if not os.path.exists(temp_path):\n",
    "        print(\"\\n📋 Creating temp file without 'seq' column...\")\n",
    "        pf = pq.ParquetFile(TRAIN_PATH)\n",
    "        cols = [c for c in pf.schema.names if c != 'seq']\n",
    "        print(f\"   Total columns: {len(pf.schema.names)}\")\n",
    "        print(f\"   Using columns: {len(cols)} (excluded 'seq')\")\n",
    "        \n",
    "        df = pd.read_parquet(TRAIN_PATH, columns=cols)\n",
    "        print(f\"   Loaded {len(df):,} rows\")\n",
    "        df.to_parquet(temp_path, index=False)\n",
    "        del df\n",
    "        gc.collect()\n",
    "        print(\"   ✅ Temp file created\")\n",
    "    else:\n",
    "        print(f\"✅ Using existing temp file: {temp_path}\")\n",
    "    \n",
    "    # Create dataset with small partitions\n",
    "    print(\"\\n📦 Creating NVTabular Dataset...\")\n",
    "    print(\"   Using 32MB partitions for memory efficiency\")\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    dataset = Dataset(\n",
    "        temp_path,\n",
    "        engine='parquet',\n",
    "        part_size='32MB'  #change size based on your environment\n",
    "    )\n",
    "    print(\"   ✅ Dataset created\")\n",
    "    \n",
    "    # Create and fit workflow\n",
    "    print(\"\\n📊 Fitting workflow...\")\n",
    "    workflow = create_workflow()\n",
    "    workflow.fit(dataset)\n",
    "    print(\"   ✅ Workflow fitted\")\n",
    "    \n",
    "    # Transform and save\n",
    "    print(f\"\\n💾 Transforming and saving to {OUTPUT_DIR}...\")\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    try:\n",
    "        workflow.transform(dataset).to_parquet(\n",
    "            output_path=OUTPUT_DIR,\n",
    "            shuffle=nvt.io.Shuffle.PER_PARTITION,\n",
    "            out_files_per_proc=8\n",
    "        )\n",
    "        \n",
    "        workflow_path = f'{OUTPUT_DIR}/workflow'\n",
    "        workflow.save(workflow_path)\n",
    "        print(f\"   ✅ Data processed and saved\")\n",
    "        print(f\"   ✅ Workflow saved to {workflow_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during processing: {e}\")\n",
    "        if os.path.exists(OUTPUT_DIR):\n",
    "            shutil.rmtree(OUTPUT_DIR)\n",
    "        raise\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    final_mem = print_memory()\n",
    "    \n",
    "    print(f\"\\n✅ Processing complete!\")\n",
    "    print(f\"   Time: {elapsed:.1f}s\")\n",
    "    print(f\"   Memory increase: +{final_mem - initial_mem:.1f}%\")\n",
    "    \n",
    "    clear_gpu_memory()\n",
    "    return OUTPUT_DIR\n",
    "\n",
    "# Process data\n",
    "processed_dir = process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54a4b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cv(processed_dir, n_folds=5):\n",
    "    \"\"\"Run stratified cross-validation\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"🔄 Stratified KFold Cross-Validation\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load processed data\n",
    "    print(\"\\n📦 Loading processed data...\")\n",
    "    start_load = time.time()\n",
    "    \n",
    "    try:\n",
    "        dataset = Dataset(processed_dir, engine='parquet', part_size='256MB')\n",
    "        print(\"   Converting to GPU DataFrame...\")\n",
    "        gdf = dataset.to_ddf().compute()\n",
    "        print(f\"   ✅ Loaded {len(gdf):,} rows x {len(gdf.columns)} columns\")\n",
    "        print(f\"   Time: {time.time() - start_load:.1f}s\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading data: {e}\")\n",
    "        return None\n",
    "    \n",
    "    print_memory()\n",
    "    \n",
    "    # Prepare data\n",
    "    print(\"\\n📊 Preparing data for XGBoost...\")\n",
    "    y = gdf['clicked'].to_numpy()\n",
    "    X = gdf.drop('clicked', axis=1)\n",
    "    \n",
    "    # Convert to float32\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype != 'float32':\n",
    "            X[col] = X[col].astype('float32')\n",
    "    \n",
    "    X_np = X.to_numpy()\n",
    "    print(f\"   Shape: {X_np.shape}\")\n",
    "    print(f\"   Features: {X.shape[1]}\")\n",
    "    print(f\"   Samples: {X.shape[0]:,}\")\n",
    "    \n",
    "    # Class distribution\n",
    "    pos_ratio = y.mean()\n",
    "    scale_pos_weight = (1 - pos_ratio) / pos_ratio\n",
    "    print(f\"\\n📊 Class distribution:\")\n",
    "    print(f\"   Positive ratio: {pos_ratio:.4f}\")\n",
    "    print(f\"   Scale pos weight: {scale_pos_weight:.2f}\")\n",
    "    \n",
    "    del X, gdf\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    # XGBoost parameters\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'max_depth': 8,\n",
    "        'learning_rate': 0.1,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'scale_pos_weight': scale_pos_weight,\n",
    "        'gpu_id': 0,\n",
    "        'verbosity': 0,\n",
    "        'seed': 42\n",
    "    }\n",
    "    \n",
    "    # Cross-validation\n",
    "    print(\"\\n🔄 Starting cross-validation...\")\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    cv_scores = []\n",
    "    cv_ap = []\n",
    "    cv_wll = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_np, y), 1):\n",
    "        print(f\"\\n📍 Fold {fold}/{n_folds}\")\n",
    "        fold_start = time.time()\n",
    "        \n",
    "        # Create DMatrix\n",
    "        print(f\"   Train: {len(train_idx):,} | Val: {len(val_idx):,}\")\n",
    "        dtrain = xgb.DMatrix(X_np[train_idx], label=y[train_idx])\n",
    "        dval = xgb.DMatrix(X_np[val_idx], label=y[val_idx])\n",
    "        \n",
    "        # Train\n",
    "        print(\"   Training...\")\n",
    "        model = xgb.train(\n",
    "            params, dtrain,\n",
    "            num_boost_round=200,\n",
    "            evals=[(dval, 'val')],\n",
    "            early_stopping_rounds=20,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = model.predict(dval)\n",
    "        score, ap, wll = calculate_competition_score(y[val_idx], y_pred)\n",
    "        \n",
    "        cv_scores.append(score)\n",
    "        cv_ap.append(ap)\n",
    "        cv_wll.append(wll)\n",
    "        \n",
    "        print(f\"   📊 Results:\")\n",
    "        print(f\"      Score: {score:.6f}\")\n",
    "        print(f\"      AP: {ap:.6f}\")\n",
    "        print(f\"      WLL: {wll:.6f}\")\n",
    "        print(f\"      Best iteration: {model.best_iteration}\")\n",
    "        print(f\"   ⏱️ Time: {time.time() - fold_start:.1f}s\")\n",
    "        \n",
    "        # Cleanup\n",
    "        del dtrain, dval, model\n",
    "        clear_gpu_memory()\n",
    "    \n",
    "    # Final results\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"📊 Final Cross-Validation Results\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\n🏆 Competition Score: {np.mean(cv_scores):.6f} ± {np.std(cv_scores):.6f}\")\n",
    "    print(f\"📈 Average Precision: {np.mean(cv_ap):.6f} ± {np.std(cv_ap):.6f}\")\n",
    "    print(f\"📉 Weighted LogLoss: {np.mean(cv_wll):.6f} ± {np.std(cv_wll):.6f}\")\n",
    "    \n",
    "    print(f\"\\nAll fold scores: {[f'{s:.6f}' for s in cv_scores]}\")\n",
    "    \n",
    "    return cv_scores\n",
    "\n",
    "# Run cross-validation\n",
    "cv_scores = run_cv(processed_dir, N_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "462825e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Final summary\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcv_scores\u001b[49m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🎉\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m35\u001b[39m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCOMPLETE!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cv_scores' is not defined"
     ]
    }
   ],
   "source": [
    "# Final summary\n",
    "if cv_scores:\n",
    "    print(\"\\n\" + \"🎉\"*35)\n",
    "    print(\"COMPLETE!\")\n",
    "    print(\"🎉\"*35)\n",
    "    print(f\"\\n✅ Final CV Score: {np.mean(cv_scores):.6f} ± {np.std(cv_scores):.6f}\")\n",
    "    print(\"✅ Full dataset processed (10.7M rows)\")\n",
    "    print(\"✅ XGBoost-optimized preprocessing (no normalization)\")\n",
    "    print(\"✅ Memory-efficient with small partitions\")\n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    print(\"\\n⚠️ Cross-validation did not complete. Please check for errors above.\")\n",
    "\n",
    "# Final cleanup\n",
    "clear_gpu_memory()\n",
    "print(\"\\n🧹 Final cleanup complete\")\n",
    "print_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69733a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "merlin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
