# CTR (Click-Through Rate) Prediction Model

(A) êµ¬ì²´ì  ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨ + API ì„¤ê³„ (ì„œë¹™ ì—”ë“œí¬ì¸íŠ¸, í”¼ì²˜ ìŠ¤í™, ë ˆì´í„´ì‹œ ì˜ˆì¸¡)
(B) ë©€í‹°íƒœìŠ¤í¬ Transformer ëª¨ë¸ì˜ PyTorch/TF êµ¬í˜„ í…œí”Œë¦¿ (í•™ìŠµ ë£¨í”„, ìº˜ë¦¬ë¸Œë ˆì´ì…˜ í¬í•¨)
(C) ì˜¤í”„ë¼ì¸ OPE + DR/IPS ì˜ˆì œ ì½”ë“œ (ë¡œê·¸ë¥¼ ë„£ìœ¼ë©´ ìƒˆë¡œìš´ ì •ì±… ê°€ì¹˜ ì¶”ì •)
(D) ì»¨í…ìŠ¤íŠ¸ ë°´ë”§(Thompson Sampling) + Lagrangian ì˜ˆì‚° ì œì•½ ì…ì°° íŒŒì´í”„ë¼ì¸ ì½”ë“œ ìƒ˜í”Œ

## Data Preprocessing & Normalization

### ë°ì´í„° ì •ê·œí™”ì˜ ì¤‘ìš”ì„±

ë”¥ëŸ¬ë‹ ëª¨ë¸, íŠ¹íˆ CTR ì˜ˆì¸¡ì—ì„œ ë°ì´í„° ì •ê·œí™”ëŠ” ì„±ëŠ¥ í–¥ìƒì— í•µì‹¬ì ì¸ ì—­í• ì„ í•©ë‹ˆë‹¤.

#### Dense Featuresì— StandardScaler ì ìš©í•˜ëŠ” ì´ìœ :

1. **Gradient ì•ˆì •ì„±**
   - ìŠ¤ì¼€ì¼ì´ ë‹¤ë¥¸ featuresë“¤ì´ gradient ê³„ì‚°ì— ë¶ˆê· í˜•ì„ ë§Œë“¦
   - ì •ê·œí™”í•˜ë©´ ëª¨ë“  featuresê°€ ë™ì¼í•œ ìŠ¤ì¼€ì¼ë¡œ í•™ìŠµì— ê¸°ì—¬

2. **í•™ìŠµ ì†ë„ í–¥ìƒ**
   - ê· ì¼í•œ ìŠ¤ì¼€ì¼ë¡œ optimizerê°€ ë” íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜ë ´
   - Learning rate ì¡°ì •ì´ ë” ì‰¬ì›Œì§

3. **Weight ì´ˆê¸°í™” íš¨ê³¼**
   - Xavier/He ì´ˆê¸°í™”ê°€ ì •ê·œí™”ëœ ì…ë ¥ì—ì„œ ë” ì˜ ì‘ë™
   - í™œì„±í™” í•¨ìˆ˜ì˜ saturation ë°©ì§€

4. **Batch Normalizationê³¼ì˜ ì‹œë„ˆì§€**
   - ì…ë ¥ ì •ê·œí™” + Batch Normìœ¼ë¡œ ë” ì•ˆì •ì ì¸ í•™ìŠµ

#### CTR ëª¨ë¸ì—ì„œ íŠ¹ë³„íˆ ì¤‘ìš”í•œ ì´ìœ :

- **Dense features**: ë³´í†µ ë§¤ìš° ë‹¤ì–‘í•œ ë²”ìœ„ (ì¡°íšŒìˆ˜, í´ë¦­ìˆ˜, ê°€ê²© ë“±)
- **Sparse features**: ì„ë² ë”©ìœ¼ë¡œ ì²˜ë¦¬ë˜ë¯€ë¡œ ë³„ë„ ì •ê·œí™” ë¶ˆí•„ìš”
- **Multi-task learning**: ì„œë¡œ ë‹¤ë¥¸ íƒœìŠ¤í¬ ê°„ ê· í˜• ìœ ì§€

### ì‚¬ìš© ë°©ë²•

```bash
# ë°ì´í„° ì „ì²˜ë¦¬ ë° train/val/test ë¶„ë¦¬
python train.py --prepare-data

# ì¼ë°˜ í›ˆë ¨ (ê¸°ì¡´ ë°ì´í„° ìˆìœ¼ë©´ ë¡œë“œ, ì—†ìœ¼ë©´ ìë™ ìƒì„±)
python train.py
```

ì „ì²˜ë¦¬ëœ ë°ì´í„°ëŠ” `./data/processed/` ë””ë ‰í† ë¦¬ì— ì €ì¥ë©ë‹ˆë‹¤:
- `train_data.parquet`: í›ˆë ¨ ë°ì´í„° (ì •ê·œí™” ì™„ë£Œ)
- `val_data.parquet`: ê²€ì¦ ë°ì´í„° (ì •ê·œí™” ì™„ë£Œ)
- `test_data.parquet`: í…ŒìŠ¤íŠ¸ ë°ì´í„° (ì •ê·œí™” ì™„ë£Œ)
- `preprocessors.pkl`: LabelEncodersì™€ StandardScaler

## TODO List - CTR ëª¨ë¸ ê°œì„  ì‚¬í•­

### ğŸ¯ High Priority

- [ ] **Feature Interaction êµ¬í˜„**
  - [ ] DeepFM ë˜ëŠ” xDeepFM ì•„í‚¤í…ì²˜ ì ìš© (FM + DNNì˜ end-to-end í•™ìŠµ)
  - [ ] DCN v2 (Deep & Cross Network v2) - ëª…ì‹œì  feature crossing
  - [ ] AutoInt - multi-head self-attention ê¸°ë°˜ feature interaction

- [ ] **Class Imbalance ì²˜ë¦¬**
  - [ ] Focal Loss êµ¬í˜„ (gamma=2 ì¶”ì²œ, CTR í´ë¦­ë¥  1-5%)
  - [ ] Class-balanced Loss (effective number ê¸°ë°˜ ë¦¬ìƒ˜í”Œë§)
  - [ ] Negative sampling with hard negative mining

- [ ] **Model Calibration (ë³´ì •)**
  - [ ] Temperature Scaling (ë‹¨ì¼ íŒŒë¼ë¯¸í„°ë¡œ ë¹ ë¥¸ ë³´ì •)
  - [ ] Platt Scaling (ë¡œì§€ìŠ¤í‹± íšŒê·€ ê¸°ë°˜)
  - [ ] Isotonic Regression (ë¹„ëª¨ìˆ˜ì  ë³´ì •)
  - [ ] Expected Calibration Error (ECE) ë° Reliability Diagramìœ¼ë¡œ í‰ê°€

### ğŸ”§ Medium Priority

- [x] **Training ìµœì í™”** (ì´ë¯¸ êµ¬í˜„ë¨)
  - [x] Model Checkpointing ì‹œìŠ¤í…œ (ì„±ëŠ¥ ê¸°ë°˜ ì €ì¥)
  - [x] Learning Rate Scheduling (CosineAnnealingWarmRestarts ì ìš©)
  - [x] Mixed Precision Training (AMP ì‚¬ìš©)
  - [x] Early Stopping with patience (validation loss ê¸°ë°˜)
  - [x] Gradient Clipping (norm=1.0 ì¶”ì²œ)
  - [x] SWA (Stochastic Weight Averaging) - ë§ˆì§€ë§‰ epochë“¤ í‰ê· 

- [x] **Regularization ê°•í™”**
  - [x] L2 Regularization (weight_decay=1e-5 ì ìš©ë¨)
  - [x] Dropout (0.1 ì ìš©ë¨)
  - [x] Label Smoothing (hard labelì„ soft labelë¡œ)
  - [x] Embedding Dropout (sparse featureìš©)
  - ~~DropConnect (weight dropout)~~ - í˜„ì¬ regularizationìœ¼ë¡œ ì¶©ë¶„, ê³¼ì í•© ì‹¬í•  ì‹œ ì‹¤í—˜ ì˜ˆì •

- [ ] **Embedding ìµœì í™”**
  - [ ] Adaptive Embedding dimension: `min(50, int(vocab_size**0.25))`
  - [ ] Hash Embedding for high cardinality features (>10K vocab)
  - [ ] Shared Embeddings for related features (user_id, session_id ë“±)
  - [ ] Embedding Regularization (L2 on embeddings)

### ğŸš€ Advanced Features

- [ ] **Context-aware Feature Engineering**
  - [ ] Temporal features (ì‹œê°„ëŒ€ë³„ CTR íŒ¨í„´, recency)
  - [ ] User behavior sequence modeling (LSTM/GRU for click history)
  - [ ] Cross-domain features (user profile + item features interaction)

- [ ] **Multi-Task Learning í™•ì¥**
  - [ ] ESMM (Entire Space Multi-Task Model) - CVR ì˜ˆì¸¡ ì¶”ê°€
  - [ ] MMOE (Multi-gate Mixture-of-Experts) - task-specific experts
  - [ ] PLE (Progressive Layered Extraction) - task separation ê°œì„ 
  - [ ] Uncertainty-based task weighting (homoscedastic uncertainty)

- [ ] **Advanced Architectures**
  - [ ] FiBiNET - Bilinear feature interaction
  - [ ] DLRM (Deep Learning Recommendation Model) - Facebook ì•„í‚¤í…ì²˜
  - [ ] BST (Behavior Sequence Transformer) - Transformer for user sequences
  - [ ] Feature Gating Network (FGN) - ë™ì  feature selection

### ğŸ“Š Monitoring & Evaluation

- [ ] **í‰ê°€ ì§€í‘œ í™•ì¥**
  - [ ] Calibration metrics (Brier Score, ECE, MCE)
  - [ ] GAUC (Group AUC) - userë³„ AUC í‰ê· 
  - [ ] NDCG@K - ranking quality
  - [ ] Business metrics (CTR, Revenue, ROAS, eCPM)

- [ ] **ì˜¨ë¼ì¸ í‰ê°€ ì¤€ë¹„**
  - [ ] A/B Test framework (treatment/control split)
  - [ ] Interleaving ì‹¤í—˜ ì„¤ê³„
  - [ ] Online model serving latency ì¸¡ì • (<100ms)

- [ ] **ëª¨ë¸ í•´ì„ì„±**
  - [ ] Integrated Gradients (attribution ê¸°ë°˜ importance)
  - [ ] SHAP values (TreeSHAP ë˜ëŠ” DeepSHAP)
  - [ ] Attention weight visualization
  - [ ] Embedding space visualization (t-SNE, UMAP)

### âš¡ ì„±ëŠ¥ ìµœì í™”

- [x] **ì¶”ë¡  ì†ë„ ê°œì„ ** (ì¼ë¶€ êµ¬í˜„ë¨)
  - [x] torch.compile() ì ìš© (PyTorch 2.0+)
  - [x] Mixed Precision Inference
  - [ ] ONNX ë³€í™˜ ë° ìµœì í™”
  - [ ] TensorRT ë˜ëŠ” OpenVINO ê°€ì†
  - [ ] Embedding ì–‘ìí™” (INT8)
  - [ ] Knowledge Distillation (í° ëª¨ë¸ â†’ ì‘ì€ ëª¨ë¸)

- [ ] **ë¶„ì‚° í•™ìŠµ**
  - [ ] DDP (Distributed Data Parallel)
  - [ ] FSDP (Fully Sharded Data Parallel) for large models
  - [ ] Gradient Accumulation (ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ)

### ğŸ”§ ì—”ì§€ë‹ˆì–´ë§ ê°œì„ 

- [ ] **ë°ì´í„° íŒŒì´í”„ë¼ì¸**
  - [ ] Feature store ì—°ë™ (Feast, Tecton)
  - [ ] Online feature serving (Redis, DynamoDB)
  - [ ] Feature versioning ì‹œìŠ¤í…œ

- [ ] **ëª¨ë¸ ì„œë¹™**
  - [ ] FastAPI ë˜ëŠ” TorchServe ê¸°ë°˜ API
  - [ ] Model versioning (A/B test ì§€ì›)
  - [ ] Batch prediction pipeline
  - [ ] Cold start ë¬¸ì œ í•´ê²° (default model)
